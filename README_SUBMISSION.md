# Documentation for the Robustness Evaluation
## Eval1: Index Performance
Evaluation corresponds to Figure 6, 7, 8 and 9 in the paper. (k=10)
In this evaluation, we compare the performance of different indices on Text-to-Image, MSSPACEV and DEEP.
For details about scripts and build instructions, please refer to the README file.
0. Install all the requirements in the `requirements_py3.10.txt` file in a virtual environment.
```shell
python3 -m venv bigann
source bigann/bin/activate
pip install -r requirements_py3.10.txt
```

1. Install indexes dockers according to the paper setup. All the dockerfiles can be found in `neurips23/ood/`
```shell
python robustness_evaluation.py --run install
```

2. Run the evaluation script to generate the results. The evaluation script is `robustness_evaluation.py` in the root directory.
All the results will be saved in the `results/neurips23/ood/` directory.
```shell
python robustness_evaluation.py --run run --count 10
```
This script will run all the six indexes with the corresponding configurations on Text-tlo-Image, MSSPACEV and DEEP datasets.
The configurations are defined in the `neurips23/ood/<index_name>/config.yaml` file.
Index code is in the `neurips23/ood/<index_name>/<index_name>.py` file.

3. Plot all the results as shown in the paper.
```shell
python robustness_evaluation.py --run plot --count 10
```
This script will plot all the results in the `results/neurips23/ood/` directory.
The results will be saved in the `results/figures/<dataset_name>/` directory.
The format of the figures name is as follows:
1. Results for each topk will be saved in a separate folder `results/figures/<dataset_name>/<topk>/`
2. The general format is `<dataset_name>-<scale>-<x-axis>-<xscale>-<y-axis>-<yscale>.png`
3. For cdf figures, the format is `<dataset_name>-<scale>-<x-axis>-<xscale>-<y-axis>-<yscale>-robustness%d.png` where the %d represents the fixed average recall %d % (90% in our evaluation). x-axis and y-axis is meaningless for the cdf figures.
4. In our evaluation, we fix the bounds of average recall for better visualization. The bounds can be set be setting the `--fix-metric k-nn` and `--min 70 --max 95`. Detailed parameters can be found in the `plot.py` file or `plot.py -h` for help.
5. In figure 9 we also show the results for K=100, it can be completed by setting `-count 100` when running the `robustness_evaluation.py` script. However, The deployment code provided by ScaNN and Puck depand on the `ds.default_count()` function, so when testing with k=100, we need to manually set the `default_count` return value to 100 in the `benchmark/dataset.py` file for the `Text2Image1B` class and the `Dataset` class.
Besides, ScaNN relies on a manual parameter configuration in `neurips23/ood/scann/scann.py`. In order to run the evaluation with k=100, we need to set the `num_neighbors` parameter in the config variable to 100, (refer to the comments in the code). We expect to make it more automatic in the future.
## Eval2: Three-Way Tradeoffs
Evaluation corresponds to Figure 10 and 11 in the paper. (k=10)
For Figure 10 in the paper:
1. 10 (a) is generated by `--k-nn --qps`, 10 (b)-(f) are generated by `--robustness@x --qps`, where x is the corresponding delta value (0.1 to 0.9). All the figures are generated by fixing the average recall to 70%-95%.
2. 11 is generated by plotting figures with two metrics while fixing the third one.
(a) is generated by `--robustness@x --qps` with `--fixed-metric k-nn`, `--min 85` or `--min 90`.
(b) is generated by `--k-nn --qps` with `--fixed-metric robustness@0.3`, `--min 0.95` or `--min 0.99`.
(c) is generated by `--k-nn --robustness-0.3@10` with `--fixed-metric qps`, `--min 50000` or `--min 100000`.
Note that the plots only show original figures seperately for the sake of clarity. We combine the figures for better illustration in the paper.
## Eval3: RAG
Evaluation corresponds to Figure 12 in the paper. The workflow is as follows:
1. MSMARCO (Emebdding with LLM-Embedder) for RAG evaluation.
2. Embedding the dataset with LLM with LLM-Embedder.
3. Filter the question set, keep the queries that LLM can answer with the embedded top-10 KNN ground truth.
4. RAG with the embedded corpus and ANN results. 
Note that the current workflow is not automated, should manually run the vector search and use the results to run RAG.
5. RAG Evaluation
For detailed scripts, please refer to the `rag/evaluate.sh` file.
## Eval4: Robustness for Index and Tuning
Evaluation of index parameters as shown in Figure 13 (HNSW) and Figure 14 (IVFFlat) in the paper.
1. HNSW: All the results have been generated in Eval1. Use `plot.py` with `--definitions faiss_hnsw` to collect the results for HNSW.
2. IVFFlat: All the results have been generated in Eval1. Use `plot.py` with `--definitions faiss-ivf` to collect the results for IVFFlat.
Unfortunately, the benchmark plotting framework does not support plotting the results of one index with different parameters using multiple lines. We collect the results and plot the results manually.
## Metric Comparison (Section 4)
We also implementd metrics mentioned in Section 4 of the paper. We get the results of common metrics in IR by replacing the axis with `mrr`, `ndcg` and `map` when calling `plot.py`.
For percentile comparison, we use `tail95`, `tail99` and `tail999` to represent the 95th, 99th and 99.9th percentile of the recall.

